---
layout: post
title: "MLOps + R: Parte 3"
subtitle: "Modelos em Produção com Sagemaker"
bigimg: /img/mlops2.gif
tags: [aws, R, mlops, deploy, rest, sagemaker]
comments: true
draft: true
output:
  html_document:
    keep_md: true
---

Já devo ter mais horas de discussão sobre python vs R do que *commits* no Github. E, apesar de gostar da treta, quando se trata de implantar modelos em produção, a linguagem de programação da solução não deveria um impedimento.

No paper [
Hidden Technical Debt in Machine Learning Systems](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf) da Google, ao ser discutido toda complexidade envolvida em implantar um modelo, o código é apresentado como um parte diminuta frente à toda problemática existente.

![](../img/ml.png)

E, assim, deve continuar sendo.

Pensando nisso, neste post, vamos entender como por em produção um modelo criado em R utilizando o Sagemaker - ferramenta de ML da AWS.

## Sagemaker Endpoints

O Sagemaker consiste em inúmeras APIs que procuram englobar o ciclo de vida de um modelo de Machine Learning. Encontramos soluções que vão desde o treinamento, seleção de modelos e deploy até o monitoramento e recalibração desses modelos.

![](../img/sage.jpeg)

Neste contexto, o deploy de modelos ocorre através dos `Sagemaker Endpoints`, serviço **gerenciado** da AWS para que permite realizar inferências (*predict*) de modelos na forma de um rest API. Por gerenciado quero dizer que toda a infraestrutura de rede, atualização das máquinas, monitoramento, até DNS é responsabilidade da AWS.

Caso você utilize o pacote python do [`Sagemaker`]() ou o [`boto3`] SDK da AWS em python, o processo de criação do endpoint a partir do modelo treinado é transparente a ponto de envolver apenas a seleção do modelo que será *deployado* e o tamanho da máquina que irá servir o modelo.

Esse processo pode ser tão transparente quanto for desejável, desde que alguns requisitos sejam atendidos.

Por baixo dos panos, um `Sagemaker Endpoint` consiste em uma máquina EC2 que executa um container docker expondo um webserver na porta 8080. Neste container estão os artefatos de modelagem (modelos e/ou dados) e códigos para lidar com as requisições ao *endpoint* e retornar o valor predito pelo modelo. Finalmente, o container deve ter um arquivo executável chamado `serve` que levanta o webserver exposto pelo endpoint.

Utilizando as soluções de prateleira do Sagemaker, nos vemos presos a containers próprios da AWS e à linguagem python, mas na minha descrição anterior em nenhum momento comento sobre qualquer de linguagem de programação.

<center>
<img src="../img/himym.gif">
</center>

## Preparando seu código para produção


## Expondo seus modelos para internet



O `Sagemaker Endpoint`, isoladamente, é acessível apenas por usuários autenticados na mesma conta da AWS em que ele foi criado. Por está razão é comum, associar a ele uma função Lambda e um API gateway para tornar seu acesso público pela internet.

![](../img/arch.png)



